[
{
	"uri": "//localhost:1313/ee/1_introduction.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will demonstrate DevOps in the cloud with AWS and JFrog and observability with Pagerduty. We will build and deploy a containerized NPM application. Using the JFrog Platform, we will execute a docker build and push, security scan the image, promotion and publish it to a repository and at the same time we will create services in Pagerduty and create webhooks in JFrog platform for artifactory, pipelines and xray to capture all those incidents and vulnerabilities . We will then deploy the image and serve the application with Amazon EKS. To automate this whole process of build, scan and deploy we will use JFrog Pipelines.\n"
},
{
	"uri": "//localhost:1313/ee/",
	"title": "JFrog DevOps Modernization Workshop",
	"tags": [],
	"description": "",
	"content": " DevOps Modernization Workshop Welcome In this workshop you will learn about the JFrog Platform and how to leverage JFrog Pipelines, Artifactory and Xray for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud on AWS.\nLearning Objectives  Understand the roles of JFrog Pipelines, Artifactory and Xray in your software delivery life cycle (SDLC). Use Local, Remote and Virtual Repositories in Artifactory. Publish and promote your software builds. Scan your artifacts and builds for security vulnerabilities. Deploy your applications to Amazon ECS or EKS automatically as part of your CI/CD pipeline.  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup.html",
	"title": "AWS Hosted Event Setup",
	"tags": [],
	"description": "",
	"content": " AWS Hosted Event Setup - Event Engine Setup Welcome to the Event Engine Setup section! This means that you are attending an AWS Hosted Workshop!! . Event Engine is a tool created at AWS that provisions AWS accounts for workshop events like this! These accounts will automatically terminate 24 hours after the workshop begins participants don\u0026rsquo;t have to worry about leaving anything on. Each workshop participant will receive their own Event Engine AWS account.\nHere is a preview of what we will be setting up:\n Get a temporary AWS account using the AWS Event Engine. Set up the AWS account and IAM roles. Provision a Cloud9 IDE instance. Get a JFrog Platform instance . Prepare our JFrog Platform instance. Clone our workshop GitHub repository which contains our code.  The next page will show you how to gain access to your Event Engine dashboard!\n"
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs.html",
	"title": "JFrog Platform with Docker Compose &amp; ECS",
	"tags": [],
	"description": "",
	"content": " JFrog Platform with Docker Compose \u0026amp; ECS In this section, we will set up our CI/CD pipeline with JFrog Pipelines. Our pipeline, will take our application and build a docker image. Push it to a Docker repository. Scan it for security vulnerabilities. Then it will promote it. Finally, it will deploy it to our AWS ECS cluster using Docker Compose. You can learn more about how Docker Compose can be used to create and deploy to Amazon ECS here.\nWe will:\n Set up the workshop code. Set up our JFrog Pipelines integrations to connect to GitHub, AWS and our Artifactory repositories. Add our CI/CD pipeline to JFrog Pipelines. Then build and deploy our app. Automatically create a new ECS cluster with Docker Compose.  In this workshop, we use Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/46_view_results.html",
	"title": "JFrog Platform with EKS",
	"tags": [],
	"description": "",
	"content": " JFrog Platform with EKS In this section, we will set up our CI/CD pipeline with JFrog Pipelines. Our pipeline, will take our application and build a docker image. Push it to a Docker repository. Scan it for security vulnerabilities. Then it will promote it. Finally, it will deploy it to our AWS EKS cluster.\nWe will:\n Set up the workshop code. Create an Amazon EKS cluster with EKSCTL and KUBECTL. Set up our JFrog Pipelines integrations to connect to GitHub, AWS and our Artifactory repositories. Add our CI/CD pipeline to JFrog Pipelines. Then build and deploy our app. Automatically deploy our app to our EKS cluster.  In this workshop, we use Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks.html",
	"title": "JFrog Platform with EKS",
	"tags": [],
	"description": "",
	"content": " JFrog Platform with EKS In this section, we will set up our CI/CD pipeline with JFrog Pipelines. Our pipeline, will take our application and build a docker image. Push it to a Docker repository. Scan it for security vulnerabilities. Then it will promote it. Finally, it will deploy it to our AWS EKS cluster.\nWe will:\n Set up the workshop code. Create an Amazon EKS cluster with EKSCTL and KUBECTL. Set up our JFrog Pipelines integrations to connect to GitHub, AWS and our Artifactory repositories. Add our CI/CD pipeline to JFrog Pipelines. Then build and deploy our app. Automatically deploy our app to our EKS cluster.  In this workshop, we use Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty.html",
	"title": "JFrog Platform with PagerDuty DevOps Observability with EKS",
	"tags": [],
	"description": "",
	"content": " JFrog Platform with PagerDuty DevOps Observability with EKS In this section, we will set up our CI/CD pipeline with JFrog Pipelines. Our pipeline, will take our application and build a docker image. Push it to a Docker repository. Scan it for security vulnerabilities. Then it will promote it. Finally, it will deploy it to our AWS EKS cluster. All along the way we will send events to PagerDuty and provide visibility to our software delivery process.\nWe will:\n Set up the workshop code. Create an Amazon EKS cluster with EKSCTL and KUBECTL. Set up out PageDuty instance to integrate with JFrog Xray and Pipelines. Set up our JFrog Pipelines integrations to connect to GitHub, AWS, our Artifactory repositories and PagerDuty. Add our CI/CD pipeline to JFrog Pipelines. Then build and deploy our app. Automatically deploy our app to our EKS cluster.  In this workshop, we use Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "//localhost:1313/ee/1_introduction/20_devops_in_cloud.html",
	"title": "DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The goal of DevOps is to allow your development teams to deliver quality software faster to your customers through continuous process improvement, leveraging the best of breed development tools and infrastructure, and utilizing software development and IT operations best practices. Your team must deliver software faster than your competitors in order to get features and fixes to your customers sooner. JFrog terms this ideal as liquid software.\n Looking forward, as release cycles get shorter and microservices get smaller, we can imagine a world in which at any one time, our systems’ software is being updated. Effectively, software will become liquid in that products and services will be connected to “software pipes” that constantly stream updates into our systems and devices; liquid software continuously and automatically updating our systems with no human intervention.\n\u0026ndash; JFrog (2017), A Vision of Liquid Software, Retrieved from https://jfrog.com/whitepaper/a-vision-of-liquid-software/\n A critical aspect of DevOps is infrastructure. Cloud computing infrastructure has allowed DevOps to advance and come closer to realizing liquid software. Cloud computing has allowed development teams to build these software pipes by:\n Using ephemeral cloud infrastructure to scale their development process and software delivery at levels not achievable with on-premise infrastructure. Providing applications on a global scale with real-time response and resiliency. Leveraging new cloud services in their application and software development processes to improve the quality, security and delivery of their applications. Allowing multi-discipline teams to collaborate in the cloud across the software lifecycle to ensure quality, security, velocity and scale of applications.  "
},
{
	"uri": "//localhost:1313/ee/1_introduction/21_continuous_integration_and_delivery.html",
	"title": "Continuous Integration and Delivery",
	"tags": [],
	"description": "",
	"content": "Continuous integration and delivery (CI/CD) is the process for which your software components are built from code, integrated, tested, released, deployed and ultimately delivered to end-users. CI/CD pipelines are the software assembly line that orchestrates the building of your software. This CI/CD pipeline line requires infrastructure. Cloud computing has allowed this infrastructure to become dynamic and ephemeral. On cloud infrastructure, your CI/CD pipelines scale up and down to meet your software delivery demands. It saves costs by providing the right amount of cloud infrastructure just as it is needed. This is further realized by using cloud-native technologies like Kubernetes and extending across clouds and on-premise datacenters. The following are some AWS cloud technologies that CI/CD pipelines can utilize:\n EC2 instances can be used as CI/CD pipeline nodes that can be dynamically spun up and down to execute pipeline tasks. EC2 spot instances can dramatically lower costs by utilizing spare capacity nodes for CI/CD pipeline tasks. Amazon Elastic Kubernetes Service (EKS) can provide a Kubernetes-based CI/CD worker node pools and allow more efficient use of compute resources. AWS Outposts can allow you to span your CI/CD pipelines from your on-premise datacenter to the cloud for hybrid and migration use cases.  "
},
{
	"uri": "//localhost:1313/ee/1_introduction/22_binary_repository_management.html",
	"title": "Binary Repository Management",
	"tags": [],
	"description": "",
	"content": "A Binary Repository Manager is a software hub that simplifies the development process for different teams across an organization by helping them to collaborate on building coherent and compatible software components. It does this by centralizing the management of all the binary artifacts generated and used by the organization, thereby overcoming the incredible complexity arising from diverse binary artifact types, their position in the overall workflow and the set of dependencies between them.\nSome of the many benefits of using a Binary Repository Manager are:\n Reliable and consistent access to remote artifacts. Reduced network traffic and optimized builds. Tight integration with build ecosystems. Custom handling of artifacts to comply with any organization’s requirements. Security and access control to artifacts and repositories. Manage licensing requirements and open source governance for use of software components. Distributing and sharing artifacts across an organization. System stability and reliability with high availability architecture. Smart search for binaries. Advanced maintenance and monitoring tools.  Cloud infrastructure has provided additional benefits. With the cloud, binary repositories can now:\n Enable replication and resiliency through the use of global data centers. Provide lower latency and improved network performance by being available closer to end-users. Provide their services at the edge of the network regionally and globally to edge devices. Utilize cloud storage for reduced costs, scalability and lower maintenance. Leverage cloud services such as security vulnerability databases to extend their functionality.  "
},
{
	"uri": "//localhost:1313/ee/1_introduction/23_dev_sec_ops.html",
	"title": "DevSecOps",
	"tags": [],
	"description": "",
	"content": "Any security issue identified by a security scanning may be reviewed by a small security team that may lack the technical knowledge. This challenge can be reduced by shifting left to the developer and operations teams, making them also responsible for security and compliance. This moves security earlier in the software delivery process. Source code, dependency and artifact security scanning are some examples of moving security into the development process. Implementing the identification of security issues earlier in the CI/CD pipeline, as well as automating security and compliance policies in the Software Development Lifecycle (SDLC), rather than using manual processes, is crucial. Moreover, organizations that leave the Sec out of DevOps, may face security and compliance issues that are closer to their release, resulting in additional costs for remediating such issues.\nAs you move your SDLC to the cloud, your DevSecOps strategy must also adapt to the cloud. As discussed previously, binary repository managers that scale globally across cloud data centers require DevSecOps tools that will likewise scale and adjust. An enterprise scale software delivery system with multiple development teams, end users and devices mean more entry points for potential security and compliance issues. Therefore, it is critical that your SLDC is well-integrated with your DevSecOps system.\n"
},
{
	"uri": "//localhost:1313/ee/1_introduction/24_jfrog_platform_overview.html",
	"title": "JFrog Platform for DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute software in the cloud. It provides DevOps teams with the tools needed to create, manage, secure and deploy software with ease. These tools cover everything from continuous integration and delivery (CI/CD), binary repository management, artifact maturity, security and vulnerability protection (DevSecOps), release management, analytics and distribution.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Mission Control and Insight is your DevOps dashboard solution for managing multiple services of Artifactory, Xray, Edge and Distribution.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.\nAll of these JFrog Platform components are designed and developed to work together out-of-the-box with minimal configuration. Management and monitoring of your software delivery lifecycle from build to distribution is accessible though a central, unified user interface. The JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow.\n "
},
{
	"uri": "//localhost:1313/ee/1_introduction/25_workshop_next_steps.html",
	"title": "Workshop Setup",
	"tags": [],
	"description": "",
	"content": "Before we get started on building, publishing and deploying our NPM application, we must set up our workshop environment. In this setup section, we will:\n Set up our AWS account and IAM roles. Provision a Cloud9 IDE instance. Get and prepare our JFrog Platform instance. Get and Setup Pagerduty instance. Clone our workshop GitHub repository which contains our code.  "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/21_aws_event_account.html",
	"title": "AWS Event: Create an AWS account",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop through an AWS hosted event.\n For an AWS hosted event, you are provided with an AWS account through the AWS Event Engine service using a 12-digit hash by event staff. This is your unique access code.\n1 . Go to https://dashboard.eventengine.run/.\n Enter the provided hash code in the text box.\n Click on the Accept Terms \u0026amp; Login button.\n Select AWS Console.\n Then select Open AWS Console.\n   This workshop supports the region us-west-2 US West (Oregon). Please select US West (Oregon) in the top right corner.  You can leave the AWS console open.\nThis AWS account will expire at the end of the workshop and any resources will automatically be de-provisioned. You will not be able to access this account after today.\n "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/22_cloud9.html",
	"title": "Set up your Cloud9 IDE",
	"tags": [],
	"description": "",
	"content": "AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It includes a code editor, debugger, and terminal. Cloud9 comes prepackaged with essential tools for popular programming languages, including JavaScript, Python, PHP, and more, so you don’t need to install files or configure your development machine to start new projects.\n Within the AWS console, use the region drop list to select us-west-2 (Oregon). The workshop script will provision the resources in this region.\n Navigate to the Cloud9 console or just search for it under the AWS services search.\n Click the Create environment button.\n For the name, enter jfrog-workshop and click Next Step.\n Select Other instance type and choose t3.medium.\n Leave all the other settings as default.\n Click Next Step.\n On the Review page, click Create environment. The Cloud9 environment will take a few minutes to provision.\n When the environment comes up, close the Welcome page tab.\n Close the lower work area tab.\n Open a new terminal tab in the main work area.\n  Your workspace should now look like this and can hide the left hand environment explorer by clicking on the left side environment tab.\nIf you don\u0026rsquo;t like this dark theme, you can change it from the View ► Themes menu.\n Cloud9 requires third-party-cookies. You can whitelist the specific domains. You are having issues with this, Ad blockers, javascript disablers, and tracking blockers should be disabled for the Cloud9 domain, or connecting to the workspace might can be impacted.\n "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/23_create_role.html",
	"title": "Create an IAM role for your workspace",
	"tags": [],
	"description": "",
	"content": " Follow this link to create an IAM role with Administrator access. Confirm that AWS service and EC2 are selected, then click Next to view permissions. Confirm that AdministratorAccess is checked, then click Next through to Review. Enter JFrog-Workshop-Admin for the role name. Click Create Role.   "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/24_attach_role_workspace.html",
	"title": "Attach the IAM role to your Workspace",
	"tags": [],
	"description": "",
	"content": " Follow this link to find your Cloud9 EC2 instance. Select the instance by clicking the checkbox, then choose Actions ► Security ► Modify IAM role.  Select your zone from top if that is not the one. Choose JFrog-Workshop-Admin from the IAM Role drop down, and click Save.   "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/25_update_iam_settings.html",
	"title": "Update IAM settings for your Workspace",
	"tags": [],
	"description": "",
	"content": " Return to your Cloud9 workspace and click the gear icon (in top right corner). Select AWS Settings. Turn off AWS managed temporary credentials. Close the Preferences tab.  Copy and run the shell commands below in your Cloud9 terminal. These shell commands will:\n   Install jq- jq is a command-line tool for parsing JSON\n Ensure temporary credentials aren’t already in place.\n Remove any existing credentials file.\n Set the region to work with our desired region.\n Validate that our IAM role is valid.\nsudo yum -y install jq rm -vf ${HOME}/.aws/credentials export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account) export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r \u0026#39;.region\u0026#39;) test -n \u0026#34;$AWS_REGION\u0026#34; \u0026amp;\u0026amp; echo AWS_REGION is \u0026#34;$AWS_REGION\u0026#34; || echo AWS_REGION is not set echo \u0026#34;export ACCOUNT_ID=${ACCOUNT_ID}\u0026#34; | tee -a ~/.bash_profile echo \u0026#34;export AWS_REGION=${AWS_REGION}\u0026#34; | tee -a ~/.bash_profile aws configure set default.region ${AWS_REGION} aws configure get default.region aws sts get-caller-identity --query Arn | grep JFrog-Workshop-Admin -q \u0026amp;\u0026amp; echo \u0026#34;IAM role valid\u0026#34; || echo \u0026#34;IAM role NOT valid\u0026#34;  If the IAM role is not valid, DO NOT PROCEED. Go back and confirm the steps on this page.\n"
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/26_jfrog_free.html",
	"title": "Get a Free JFrog Platform Instance",
	"tags": [],
	"description": "",
	"content": "If you do not have access to a JFrog Platform instance, use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory and Xray.\nWhen signing up for the JFrog Platform Cloud Free Tier, ensure that you select AWS and the US West 2 (Oregon) region.\n  JFrog Platform Cloud Free Tier   "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/28_docker_repositories.html",
	"title": "Set Up Docker Repositories",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance at the top right, enable the drop down menu and select Quick Setup.\n On the Create Repositories dialog, choose Docker and click Next.\n Next, enter workshop for the Repositories Prefix.\n Click Create. This will create the following docker repositories:\n workshop-docker-local workshop-docker-remote workshop-docker   Local repositories are physical, locally-managed repositories into which you can deploy artifacts. These are repositories that are local to the JFrog Artifactory instance. A remote repository serves as a caching proxy for a repository managed at a remote URL (which may itself be another Artifactory remote repository). A virtual repository (or \u0026ldquo;repository group\u0026rdquo;) aggregates several repositories with the same package type under a common URL. A virtual repository can aggregate local and remote repositories.  Next, let\u0026rsquo;s create another docker repository to represent production images. Click the Add Repositories button and select Local Repository.\n Select Docker for the Package Type.\n Name this docker repository workshop-docker-prod-local. Click Save \u0026amp; Finish.\n Click on the Virtual tab under Repositories.\n Click on the workshop-docker virtual repository.\n Move the new workshop-docker-prod-local repository under Selected Repositories.\n Under Default Deployment Repository, select workshop-docker-prod-local as the default deployment repository. This means that image deployments from virtual repository workshop-docker will actually deploy from the local repository workshop-docker-prod-local.\n Click Save \u0026amp; Finish.\n Under the Local tab under Repositories, select the workshop-docker-local repository.\n Scroll down and check Enable Indexing in Xray. This tells Xray to automatically scan this repository.\n Click Save \u0026amp; Finish.\n  Congratulations! You have set up your Docker repositories. Now let\u0026rsquo;s configure our security policies for Xray.\n"
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/27_pipelines_setup.html",
	"title": "Set Up Pipelines",
	"tags": [],
	"description": "",
	"content": "In your JFrog Platform instance, go to Application \u0026gt; Pipelines and follow the instructions to activate JFrog Pipelines. This will take a few minutes.\nNote: You might have to put your credit card info which is just for verification, it will not get charged.\nMove onto the next step while JFrog Pipelines activates.\n"
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/29_xray_setup.html",
	"title": "Set Up Xray Security",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Administration \u0026gt; Xray \u0026gt; Watches \u0026amp; Policies.\n Click Create a Policy.\n Call the security policy, High-Severity.\n Click on New Rule.\n Name the rule High-Severity and select High for the Minimal Severity. Click Save.\n Click Create to create this new security policy.\n Click on the Watches tab under Watches \u0026amp; Policies.\n Click on Set up a Watch.\n Name the new watch Docker-Scanning.\n Click Add Repositories.\n Move the repositories workshop-docker-local and workshop-docker-prod-local to the Included Repositories.\n Click Save.\n Click Manage Policies.\n Move the High-Severity policy to the Included Policy.\n Click Save.\n Click Create to create the new watch. This watch will scan the workshop-docker-local and workshop-docker-prod-local Docker repositories for new images and check for high severity security vulnerabilities.\n  JFrog Xray scans your artifacts, builds and release bundles for OSS components, and detects security vulnerabilities and licenses in your software components. Policies and Watches allow you to enforce your organization governance standards. Setup up your Policies and Watches to reflect standard governance behaviour specifications for your organization across your software components.\n "
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs/31_download_code.html",
	"title": "Download the Code",
	"tags": [],
	"description": "",
	"content": "The code is located at https://github.com/jfrogtraining/aws-ecs-docker-compose-workshop GitHub repository. We will clone this repo locally in order to pull the required workshop files and scripts.\n Go to https://github.com/jfrogtraining/aws-ecs-docker-compose-workshop and fork this repository to your GitHub account.\n In your Cloud9 terminal, clone this repository to your local directory with the following command.\n   git clone https://github.com/\u0026lt;github username\u0026gt;/aws-ecs-docker-compose-workshop.git \n"
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs/32_pipeline_integrations.html",
	"title": "Set up our JFrog Pipelines Integrations",
	"tags": [],
	"description": "",
	"content": "Our CI/CD pipeline requires access to GitHub to pull our code, access to JFrog Artifactory to deploy our Docker image and access to AWS to deploy to ECS. We will set up JFrog Pipelines integrations to enable these.\n![Pipelines Integration](/images/pipeline-integrations-diagram.png) An Integration connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.\n You may have already created these integrations in previous steps. If so, you can reuse them and do not need to recreate them. JFrog Pipelines allows you to share integrations and resources across pipelines.\n  In your JFrog Platform instance, go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration.\n For the Name, enter artifactory_integration.\n For Integration Type, select Artifactory.\n Click Get API Key to generate an API key.\n Click Test connection to validate.\n Click Create to create the integration.\n Click Add an Integration again.\n For the Name, enter github_integration.\n For Integration Type, select GitHub.\n Copy and paste your GitHub personal access token. Ensure it has these minimum GitHub permissions:\n repo (all) admin:repo_hook (read, write) admin:public_key (read, write)  Click Test connection to validate.\n Click Create to create the integration.\n In your Cloud9 terminal, execute the following commands to create a AWS user and access key ID and secret access key.\naws iam create-user --user-name workshopuser aws iam attach-user-policy --user-name workshopuser --policy-arn arn:aws:iam::aws:policy/AdministratorAccess aws iam create-access-key --user-name workshopuser Copy the output of these commands.\n Go back to your JFrog Platform instance and go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration again.\n For the Name, enter aws_integration.\n For Integration Type, select AWS.\n For the Access Key Id and the Secret Access Key, enter the values from above.\n Click Create to create the integration.\n  Congratulations! We have created the integrations that are required for our CI/CD pipeline.\n"
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs/33_update_pipeline.html",
	"title": "Update our Pipeline",
	"tags": [],
	"description": "",
	"content": "We need to make an update to our CI/CD pipeline in order to use your JFrog Platform instance. The CI/CD pipeline is defined in pipelines.yml. This pipeline file is parameterized with a values.yml file. We need to update this file.\n In your Cloud9 terminal, use the editor and view the pipelines.yml file. View and understand the steps. Note the parameterized values.\n The last step in the pipeline, ecs_deploy, deploys our application to an ECS cluster using docker compose with a special ECS context. This is achieved with the deploy.sh script. View this file in the editor.\n In the editor, select the values.yml file and updated the parameterized values to point to your JFrog Platform instance and your forked repo. Save the changes.\n In your Cloud9 terminal, commit these changes.\ngit add . git commit -m \u0026#39;Updated values.yml.\u0026#39; Next, push these updates. When prompted for a username and password, use your GitHub username and personal access token/password.\n   git push origin master \nWe are now ready to add your CI/CD pipeline and execute!\n"
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs/34_build_publish_deploy.html",
	"title": "Build, Publish and Deploy to ECS",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Application \u0026gt; Pipelines \u0026gt; Pipeline Sources.\n Click Add a pipeline source.\n Click Add Pipeline Source at the top right and select From YAML.\n For SCM Provider Integration, select the github_integration that you created previously.\n For Repository Full Name, select your forked /aws-ecs-docker-compose-workshop.\n For Branch, select master.\n Leave Pipeline Config File Filter as pipelines.yml.\n Click Create Source. JFrog Pipelines will process the CI/CD pipeline. The status should show Not Synced then Syncing and then Success.\n Go to Application \u0026gt; Pipelines \u0026gt; My Pipelines. Notice that your pipeline has a status of Not Built.\n Click on your pipeline, aws_ecs_docker_compose_workshop_app_build.\n Click on the app_docker_build step and trigger the step to execute the pipeline. JFrog Pipelines will allocate build nodes and execute your pipeline.\n It will take a few minutes to execute. Click on the Run to monitor the status of each step.\n Use the pulldown to select each step and view the logs.\n When the pipeline execution completes (approximately 5-10 minutes), view the log for the ecs_deploy step.\n While this pipeline is executing, go to the CloudFormation console to watch the CloudFormation stack create the new ECS cluster. You can also go to the ECS console to observe the new cluster.\n Scroll down to get the PORTS value. Enter the value into your browser to view the application!\n  "
},
{
	"uri": "//localhost:1313/ee/3_deploy_ecs/35_view_results.html",
	"title": "View Results in JFrog",
	"tags": [],
	"description": "",
	"content": "We have built and published our Docker image. Let\u0026rsquo;s view these results in the JFrog Platform.\n Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages. Search for the name of the Docker image that was built in your workshop. For the ECS build, search for ecs-docker-compose-workshop-app. For the EKS build, search for eks-workshop-app. Click on the Docker image listing.  This will show a list of the versions. Click on the latest version that was built.  In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.  Click on any violation to see the details and impact in the Issue Details tab.  Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this. Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries.\n  Close the Issue Details tab.\n View the Docker configuration for the image in the Docker Layers tab.\n On the Builds tab, click on npm_build in the list.  Then click on your most recent build.\n In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using JFrog Pipelines, Artifactory and Xray. With the JFrog Platform, you can view the results of your software build from versions to vulnerabilities.\n"
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/41_download_code.html",
	"title": "Download the Workshop Code",
	"tags": [],
	"description": "",
	"content": "The code is located at https://github.com/jfrogtraining/aws-eks-workshop GitHub repository. We will clone this repo locally in order to pull the required workshop files.\n Go to https://github.com/jfrogtraining/aws-eks-workshop and fork this repository to your GitHub account.\n In your Cloud9 terminal, clone this repository to your local directory with the following command.\n   git clone https://github.com/\u0026lt;github username\u0026gt;/aws-eks-workshop.git \n"
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/42_create_eks_cluster.html",
	"title": "Create Your EKS Cluster",
	"tags": [],
	"description": "",
	"content": "We will use the EKSCTL command line tool to create an Amazon Elastic Kubernetes (EKS) cluster. This powerful tool allows you to manage many aspects of your EKS cluster through simple commands. When using EKSCTL, you can still manage your cluster in the Amazon EKS dashboard in the AWS console.\nAmazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. Amazon EKS helps you provide highly-available and secure clusters and automates key tasks such as patching, node provisioning, and updates.\n  In your Cloud9 terminal, follow these instructions to install EKSCTL on Linux. Follow these instructions to install KUBECTL on Linux. Next, execute the following command to create your EKS cluster. It is that easy! This will take a few minutes.  eksctl create cluster --name jfrogeksworkshop --region us-west-2 --managed \n Update your kubeconfig to add the credentials for your new cluster.  aws eks update-kubeconfig --name jfrogeksworkshop --region us-west-2 \n Execute the following command to print out your kubeconfig. Copy the output to your notepad. We will need this later to allow JFrog Pipeline to access the cluster.  cat /home/ec2-user/.kube/config   Execute the following command to create a namespace where we will deploy our application.  kubectl create namespace aws-eks-workshop \n In your JFrog Platform instance, go to Administration \u0026gt; Identity and Access \u0026gt; Access Tokens.\n Click + Generate Admin Token.  Leave the defaults and click Generate.  Copy the Username and Access Token.\n Go back to your Cloud9 terminal, and execute the following command to create an image pull secret that will be used to pull and deploy your image.\nkubectl create secret docker-registry artifactory-secret --namespace aws-eks-workshop --docker-server=\u0026lt;your JFrog Platform instance domain\u0026gt; --docker-username=\u0026lt;username from above\u0026gt; --docker-password=\u0026lt;token from above\u0026gt;  Example:\n kubectl create secret docker-registry artifactory-secret --namespace aws-eks-workshop --docker-server=myjfrog.jfrog.io --docker-username=jefff --docker-password=xxxxx..."
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/43_pipeline_integrations.html",
	"title": "Set up our JFrog Pipelines Integrations",
	"tags": [],
	"description": "",
	"content": "Our CI/CD pipeline requires access to GitHub to pull our code, access to JFrog Artifactory to deploy our Docker image and access to AWS to deploy to EKS. We will set up JFrog Pipelines integrations to enable these.\nAn Integration connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.\n You may have already created these integrations in previous steps. If so, you can reuse them and do not need to recreate them. JFrog Pipelines allows you to share integrations and resources across pipelines.\n  In your JFrog Platform instance, go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration.\n For the Name, enter artifactory_integration.\n For Integration Type, select Artifactory.\n Click Get API Key to generate an API key.\n Click Test connection to validate.\n Click Create to create the integration.  Click Add an Integration again.\n For the Name, enter github_integration.\n For Integration Type, select GitHub.\n Copy and paste your GitHub personal access token. Ensure it has these minimum GitHub permissions:\n repo (all) admin:repo_hook (read, write) admin:public_key (read, write)  Click Test connection to validate.\n Click Create to create the integration.  In your Cloud9 terminal, execute the following commands to create a AWS user and access key ID and secret access key.\naws iam create-user --user-name workshopuser aws iam attach-user-policy --user-name workshopuser --policy-arn arn:aws:iam::aws:policy/AdministratorAccess aws iam create-access-key --user-name workshopuser Copy the output of these commands.  Go back to your JFrog Platform instance and go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration again.\n For the Name, enter aws_integration.\n For Integration Type, select AWS.\n For the Access Key Id and the Secret Access Key, enter the values from above.\n Click Create to create the integration.  Click Add an Integration again.\n For the Name, enter eks_integration.\n For Integration Type, select Kubernetes.\n Paste in the Kubeconfig output from the steps where you created your EKS cluster.\n Click Create to create the integration. Congratulations! We have created the integrations that are required for our CI/CD pipeline.\n  "
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/44_update_pipeline.html",
	"title": "Update our Pipeline",
	"tags": [],
	"description": "",
	"content": "We need to make an update to our CI/CD pipeline in order to use your JFrog Platform instance. The CI/CD pipeline is defined in pipelines.yml. This pipeline file is parameterized with a values.yml file. We need to update this file.\n In your Cloud9 terminal, use the editor and view the pipelines.yml file. View and understand the steps. Note the parameterized values.  The last step in the pipeline, eks_deploy, deploys our application to the EKS cluster we created earlier. This is achieved with the K8s deployment.yml manifest. View this file in the editor.  In the editor, select the values.yml file and updated the parameterized values to point to your JFrog Platform instance and your forked repo. Save the changes.  In your Cloud9 terminal, commit these changes.\ngit add . git commit -m \u0026#39;Updated values.yml.\u0026#39; Next, push these updates. When prompted for a username and password, use your GitHub username and personal access token/password.\n git push origin master \nWe are now ready to add your CI/CD pipeline and execute!\n  "
},
{
	"uri": "//localhost:1313/ee/4_deploy_eks/45_build_publish_deploy.html",
	"title": "Build, Publish and Deploy to EKS",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Application \u0026gt; Pipelines \u0026gt; Pipeline Sources.  Click Add a pipeline source.\n Click Add Pipeline Source at the top right and select From YAML.  For SCM Provider Integration, select the github_integration that you created previously.\n For Repository Full Name, select your forked /aws-eks-workshop.\n For Branch, select master.\n Leave Pipeline Config File Filter as pipelines.yml.  Click Create Source. JFrog Pipelines will process the CI/CD pipeline. The status should show Not Synced then Syncing and then Success.  Go to Application \u0026gt; Pipelines \u0026gt; My Pipelines. Notice that your pipeline has a status of Not Built.  Click on your pipeline, eks_workshop_app_build.  Click on the app_docker_build step and trigger the step to execute the pipeline. JFrog Pipelines will allocate build nodes and execute your pipeline.  It will take a few minutes to execute. Click on the Run to monitor the status of each step.\n Use the pulldown to select each step and view the logs.  When the pipeline execution completes (approximately 5-10 minutes), view the log for the eks_deploy step.\n While this pipeline is executing, go to the EKS console to observe the new cluster.\n Scroll down to get the $url value. Enter the value into your browser to view the application!   "
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/51_download_code.html",
	"title": "Download the Workshop Code",
	"tags": [],
	"description": "",
	"content": "The code is located at https://github.com/jfrogtraining/pagerduty-workshop GitHub repository. We will clone this repo locally in order to pull the required workshop files.\n Go to https://github.com/jfrogtraining/pagerduty-workshop and fork this repository to your GitHub account.\n In your Cloud9 terminal, clone this repository to your local directory with the following command.\ngit clone git@github.com:\u0026lt;github username\u0026gt;/pagerduty-workshop.git  "
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/52_create_eks_cluster.html",
	"title": "Create Your EKS Cluster",
	"tags": [],
	"description": "",
	"content": "We will use the EKSCTL command line tool to create an Amazon Elastic Kubernetes (EKS) cluster. This powerful tool allows you to manage many aspects of your EKS cluster through simple commands. When using EKSCTL, you can still manage your cluster in the Amazon EKS dashboard in the AWS console.\nAmazon Elastic Kubernetes Service (Amazon EKS) is a managed service that you can use to run Kubernetes on AWS without needing to install, operate, and maintain your own Kubernetes control plane or nodes. Kubernetes is an open-source system for automating the deployment, scaling, and management of containerized applications. Amazon Elastic Kubernetes Service (Amazon EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS cloud or on-premises. Amazon EKS helps you provide highly-available and secure clusters and automates key tasks such as patching, node provisioning, and updates.\n  In your Cloud9 terminal, follow these instructions to install EKSCTL on Linux. Follow these instructions to install KUBECTL on Linux. Next, execute the following command to create your EKS cluster. It is that easy! This will take a few minutes.  eksctl create cluster --name workshop --region us-west-2 --managed  Update your kubeconfig to add the credentials for your new cluster with the following command.  aws eks update-kubeconfig --name workshop --region us-west-2  Execute the following command to print out your kubeconfig. Copy the output to your notepad. We will need this later to allow JFrog Pipeline to access the cluster.  cat /home/ec2-user/.kube/config   Execute the following command to create a namespace where we will deploy our application.  kubectl create namespace workshop \n In your JFrog Platform instance, go to Administration \u0026gt; Identity and Access \u0026gt; Access Tokens.\n Click + Generate Token.  Select Token Scope \u0026gt; Admin and put the UserName. Leave the defaults and click Generate.  Copy the Username and Access Token.\n Go back to your Cloud9 terminal, and execute the following command to create an image pull secret that will be used to pull and deploy your image.\nkubectl create secret docker-registry artifactory-secret --namespace workshop --docker-server=\u0026lt;your JFrog Platform instance domain\u0026gt; --docker-username=\u0026lt;username from above\u0026gt; --docker-password=\u0026lt;token from above\u0026gt;  Example:\n kubectl create secret docker-registry artifactory-secret --namespace workshop --docker-server=myjfrog.jfrog.io --docker-username=mitalib --docker-password=xxxxx..."
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/53_set_up_pagerduty.html",
	"title": "Set up PagerDuty",
	"tags": [],
	"description": "",
	"content": "Next we will set up your PagerDuty Developer Platform. If you do not yet have a developer platform, you can sign up and get a free developer platform instance here.\nPagerDuty provides many ways for developers to integrate with their platform. You can learn more through their developer documentation.\n  In your PagerDuty, go to Services \u0026gt; Service Directory. A service may represent an application, component, or team you wish to open incidents against. Services contain integrations, as well as determine the routing and incident settings for events triggered by integrations associated with the service.\n  We will create two services: (1) Xray Vulnerabilities and (2) JFrog Pipelines and Artifactory Events. Click **+ New Service **. Name the service Xray Vulnerabilities and click Next.  Choose Generate an Escalation Policy and click Next.  Choose Intelligent for the Alert Grouping and click Next.  For Integrations, search for JFrog Xray Notifications and click Create Service.  Copy the Integration URL for later.  Let\u0026rsquo;s now create a service for JFrog Pipelines and Artifactory events. Click **+ New Service **. Name the service Artifactory and Pipelines Events and click Next. Choose Generate an Escalation Policy and click Next. Choose Intelligent for the Alert Grouping and click Next. For Integrations, search and select JFrog Pipelines Changes and JFrog Artifactory Notifications click Create Service.  Copy the Integration Key for pipelines and Integration URL for artifactory to use later.   Congratulations! You have set up PagerDuty with the JFrog Xray and Pipelines integrations.\n"
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/54_pipeline_integrations.html",
	"title": "Set up our JFrog Pipelines Integrations",
	"tags": [],
	"description": "",
	"content": "Our CI/CD pipeline requires access to GitHub to pull our code, access to JFrog Artifactory to deploy our Docker image and access to AWS to deploy to EKS. We will set up JFrog Pipelines integrations to enable these.\nAn Integration connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.\n You may have already created these integrations in previous steps. If so, you can reuse them and do not need to recreate them. JFrog Pipelines allows you to share integrations and resources across pipelines.\n  In your JFrog Platform instance, go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration.\n For the Name, enter artifactory_integration.\n For Integration Type, select Artifactory.\n Click Get API Key to generate an API key.\n Click Test connection to validate.\n Click Create to create the integration.  Click Add an Integration again.\n For the Name, enter github_integration.\n For Integration Type, select GitHub.\n Copy and paste your GitHub personal access token. Ensure it has these minimum GitHub permissions:\n repo (all) admin:repo_hook (read, write) admin:public_key (read, write)  Click Test connection to validate.\n Click Create to create the integration.  In your Cloud9 terminal, execute the following commands to create a AWS user and access key ID and secret access key.\naws iam create-user --user-name workshopuser aws iam attach-user-policy --user-name workshopuser --policy-arn arn:aws:iam::aws:policy/AdministratorAccess aws iam create-access-key --user-name workshopuser Copy the output of these commands and save it somewhere to put in Jfrog Platform.  Now you will also have to update the configmap with the above user created in order to have access to EKS clusters in JFrog Platform. In Cloud9 terminal execute the following command to edit configmap.\nkubectl edit -n kube-system configmap/aws-auth Add the MapUsers section as shown in highlighted red box\nmapusers: | - userarn: arn:aws:iam::\u0026lt;id\u0026gt;:user/workshopuser username: workshopuser groups: - system:masters  Go back to your JFrog Platform instance and go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n Click Add an Integration again.\n For the Name, enter aws_integration.\n For Integration Type, select AWS.\n For the Access Key Id and the Secret Access Key, enter the values from above.\n Click Create to create the integration.  Click Add an Integration again.\n For the Name, enter eks_integration.\n For Integration Type, select Kubernetes.\n Paste in the Kubeconfig output from the steps where you created your EKS cluster.\n Click Create to create the integration.  Click Add an Integration again.\n For the Name, enter pagerduty_integration.\n For Integration Type, select PagerDuty Events.\n Enter the PagerDuty Pipelines Integration Key created in the prior steps for Service Integration/routing key.  Click Create.\n Go to Administration \u0026gt; Xray \u0026gt; Settings.\n Click on Webhooks in the General tile.\n Create a New Webhook.\n Enter Xray PagerDuty for the Webhook Name\n Enter the PagerDuty Xray Integration URL for the URL. ex: https://events.pagerduty.com/integration/\u0026lt; integration id \u0026gt;/enqueue\n Click Create.  Go to Administration \u0026gt; Xray \u0026gt; Watches \u0026amp; Policies.\n Click on the High-Severity policy that you created earlier.\n Then click on the edit icon for the High-Severity rule that you created earlier.  Scroll down and enable Trigger Webhook and select Xray Pagerduty, the webhook that you created in the previous steps.\n Click Save to save the rule.  Click Save to save the policy.\n Similarly, now create a webhook for artifactory also. Go to Administration \u0026gt; General \u0026gt; Webhooks and click on New Webhook.\n Enter Artifactory Pagerduty Docker for the Webhook Name\n Enter the PagerDuty Artifactory Integration URL for the URL and select Docker from the Events and check all docker events.  Repeat steps 46-48 to create one more webhook Artifactory Pagerduty Artifact for the events Artifacts.\n  Congratulations! We have created the integrations that are required for our CI/CD pipeline.\n"
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/55_update_pipeline.html",
	"title": "Update our Pipeline",
	"tags": [],
	"description": "",
	"content": "We need to make an update to our CI/CD pipeline in order to use your JFrog Platform instance. The CI/CD pipeline is defined in pipelines.yml. This pipeline file is parameterized with a values.yml file. We need to update this file.\n In your Cloud9 terminal, use the editor and view the pipelines.yml file. View and understand the steps. Note the parameterized values.  The last step in the pipeline, eks_deploy, deploys our application to the EKS cluster we created earlier. This is achieved with the K8s deployment.yml manifest. View this file in the editor.  In the editor, select the values.yml file and updated the parameterized values to point to your JFrog Platform instance and your forked repo. Save the changes.\nArtifactory: server: partnership.jfrog.io #change to your servername intName: artifactory_integration devRepo: workshop-docker-local prodRepo: workshop-docker-prod-local PagerDuty: intName: pagerduty_integration pipelinesRoutingKey: xxxxxx #change to your pagerduty pipelines integration key GitHub: path: jfrogtraining/pagerduty-workshop #change to your github username gitProvider: github_integration AWS: intName: aws_integration eks: eks_integration region: us-west-2 app: dockerImageName: partnership.jfrog.io/workshop-docker/pagerduty-workshop-app #change to your servername dockerFileLocation: workshop-app buildName: pagerduty-workshop-app In your Cloud9 terminal, commit these changes.\ngit add . git commit -m \u0026#39;Updated values.yml.\u0026#39; Next, push these updates. When prompted for a username and password, use your GitHub username and personal access token/password.\ngit push origin master We are now ready to add your CI/CD pipeline and execute!\n  "
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/56_build_publish_deploy.html",
	"title": "Build, Publish and Deploy to EKS",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Application \u0026gt; Pipelines \u0026gt; Pipeline Sources.  Click Add a pipeline source.\n Click Add Pipeline Source at the top right and select From YAML.  For SCM Provider Integration, select the github_integration that you created previously.\n For Repository Full Name, select your forked /pagerduty-workshop.\n For Branch, select master.\n Leave Pipeline Config File Filter as pipelines.yml.  Click Create Source. JFrog Pipelines will process the CI/CD pipeline. The status should show Not Synced then Syncing and then Success.  Go to Application \u0026gt; Pipelines \u0026gt; My Pipelines. Notice that your pipeline has a status of Not Built.  Click on your pipeline, workshop_app_build.  Click on the app_docker_build step and trigger the step to execute the pipeline. JFrog Pipelines will allocate build nodes and execute your pipeline.  It will take a few minutes to execute. Click on the Run to monitor the status of each step.\n Use the pulldown to select each step and view the logs.  When the pipeline execution completes (approximately 5-10 minutes), view the log for the eks_deploy step.\n While this pipeline is executing, go to the EKS console to observe the new cluster.\n Scroll down to get the $url value. Enter the value into your browser to view the application!   "
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/57_view_results.html",
	"title": "View Results in JFrog",
	"tags": [],
	"description": "",
	"content": "We have built and published our Docker image. Let\u0026rsquo;s view these results in the JFrog Platform.\n Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages. Search for the name of the Docker image that was built in your workshop pagerduty-workshop-app. Click on the Docker image listing. This will show a list of the versions. Click on the latest version that was built.  In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.  Click on any violation to see the details and impact in the Issue Details tab.  Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this. Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries.\n  Close the Issue Details tab.\n View the Docker configuration for the image in the Docker Layers tab.\n On the Builds tab, click on npm_build in the list.  Then click on your most recent build.\n In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using JFrog Pipelines, Artifactory and Xray. With the JFrog Platform, you can view the results of your software build from versions to vulnerabilities.\n"
},
{
	"uri": "//localhost:1313/ee/5_deploy_eks_pagerduty/58_view_pagerduty.html",
	"title": "View Events in PagerDuty",
	"tags": [],
	"description": "",
	"content": " Go to your Pagerduty instance, navigate to Services \u0026gt; Service Directory and search and select Artifactory and Pipelines evnts. You should be able to see all the change events and notifications from Pipelines and Artifactory.  Now click on any pipeline event. This will give you information about that Jfrog pipeline event and if you click on View in Build Pipeline , it will take you to that particular pipeline in JFrog instance.  Now select any artifactory event. This will give you information about that event like docker image name, which repo it was deployed etc.  On the service page you can assign that to any team, connected to any communication channel like slack or view audit trail and many more.  Now ho back to Service Directory , search and select Xray Vulnerabilities. You should be able to see lots of Incidents telling about vulnerabilities. You can achnowledge, assign or take any action based on priority.  Select any of the Incident and you can see more details about it like severity, impacted artifacts, CVE etc.   "
},
{
	"uri": "//localhost:1313/ee/099_survey.html",
	"title": "Survey",
	"tags": [],
	"description": "",
	"content": " Survey To help us improve this product, please take some time to fill out the  Survey  "
},
{
	"uri": "//localhost:1313/ee/2_event_engine_setup/210_workshop_next_steps.html",
	"title": "Workshop Next Steps",
	"tags": [],
	"description": "",
	"content": "You are now ready to build an application with the JFrog Platform. Please choose which AWS service you wish to deploy to.\nAmazon ECS using Docker Compose Amazon EKS JFrog Platform with PagerDuty DevOps Observability on EKS\n"
},
{
	"uri": "//localhost:1313/ee/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/ee/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": " Your JFrog Platform Instance - The JFrog Platform instance that you used in this workshop will automatically be destroyed after the workshop. There isn\u0026rsquo;t anything you need to do. If you would like keep it, you can upgrade to one of the premium plans. Do this by clicking on the Upgrade button.  Amazon Resources\n Amazon ECS Resources - To cleanup your Amazon ECS resources, go to your npm-app-cluster in your Amazon ECS console and click Delete Cluster.  AWS Secrets - To cleanup your Amazon secrets, go to your secrets list in the Secrets Manager console. Select the secret and click Delete secret.  IAM Roles - To delete your ecsWorkshop role, go to your role in the IAM console and click Delete role.  IAM Policy - To delete your ecsAccessToSecrets policy, go to your policy in the IAM console. Select it and click Delete policy.    "
},
{
	"uri": "//localhost:1313/ee/resources.html",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "//localhost:1313/ee/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]